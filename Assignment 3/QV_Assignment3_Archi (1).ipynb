{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cveR_jqy2Tm"
      },
      "outputs": [],
      "source": [
        "#Downloading dataset\n",
        "import os\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d raimiazeezbabatunde/candle-image-data\n",
        "!unzip -q candle-image-data.zip -d ./candlestick_data\n",
        "print(\"DONE\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEPS:\n",
        "1. Data collection\n",
        "2. Preprocessing\n",
        "3. Building the model\n",
        "4. Training the model\n",
        "5. Evaluating its performance\n",
        "6. Visualizing the results\n"
      ],
      "metadata": {
        "id": "fa9SmoaX4bES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Libraries\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n"
      ],
      "metadata": {
        "id": "_-n46hUU7QJb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Resizing data and Data Augmentation\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((64,64)),\n",
        "    transforms.ColorJitter(brightness=0.2),\n",
        "    transforms.RandomCrop(64, padding=4),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((64,64)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Loading dataset\n",
        "train_dataset = ImageFolder(\n",
        "    root = '/content/candlestick_data',\n",
        "    transform=train_transform\n",
        ")\n",
        "test_dataset = ImageFolder(\n",
        "    root = '/content/candlestick_data',\n",
        "    transform=test_transform\n",
        ")\n",
        "\n",
        "# Train-Test split\n",
        "train_size = int(0.8*len(train_dataset))\n",
        "test_size = len(train_dataset) - train_size\n",
        "\n",
        "generator = torch.Generator().manual_seed(11)\n",
        "\n",
        "train_data, _ = random_split(train_dataset, [train_size, test_size], generator=generator)\n",
        "_, test_data   = random_split(train_dataset,  [train_size, test_size], generator=generator)\n",
        "\n",
        "\n",
        "print(f\"Training Data: {len(train_data)} images\")\n",
        "print(f\"Testing Data: {len(test_data)} images\")"
      ],
      "metadata": {
        "id": "ugQMZC8ahkXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "\n",
        "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "for X,y in train_dataloader:\n",
        "  print(f\"Shape of X: {X.shape}\")  #[batchsize, channels(rbg), height, width]\n",
        "  print(f\"Shape of y: {y.shape}\")\n",
        "  break"
      ],
      "metadata": {
        "id": "KWvqtg-Ieiyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "raHID3YHviqh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DEFINING THE CNN MODEL"
      ],
      "metadata": {
        "id": "HmnK9wLo0dVy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining CNN model\n",
        "class CNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # Feature Extractor\n",
        "    self.feature_extractor = nn.Sequential(\n",
        "        # Layer 1\n",
        "        nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2),\n",
        "        # Layer 2\n",
        "        nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2),\n",
        "        # Layer 3\n",
        "        nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2),\n",
        "        # Flattening\n",
        "        nn.Flatten()\n",
        "    )\n",
        "    # Classifier\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Dropout(p=0.2),\n",
        "        nn.Linear(32*8*8, 2)\n",
        "    )\n",
        "  def forward(self, x):\n",
        "        x = self.feature_extractor(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "h5N7SeVFj34n"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TRAINING THE MODEL"
      ],
      "metadata": {
        "id": "sgOfeqp-0hR-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "Elu59fnQ1EeT"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Deciding values of Hyperparamters\n",
        "lr = 0.001\n",
        "num_epochs = 50"
      ],
      "metadata": {
        "id": "fGqQwdfA2qwT"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "QWTok6-puSqf"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training and Evaluation of model\n",
        "train_losses = []\n",
        "val_accuracies = []\n",
        "\n",
        "model = CNN().to(device)\n",
        "class_weights = torch.tensor([0.83, 0.17]).to(device)\n",
        "# Choosing loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss(weight = class_weights)\n",
        "optimizer = optim.Adam(model.parameters(), lr)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  model.train()\n",
        "  running_loss = 0.0\n",
        "  for X,y in train_dataloader:\n",
        "    X = X.to(device)\n",
        "    y = y.to(device)\n",
        "    # Forward pass\n",
        "    pred = model(X)\n",
        "    # Evaluate loss\n",
        "    loss = criterion(pred, y)\n",
        "    # Backpropagation\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "\n",
        "  avg_loss = running_loss / len(train_dataloader)\n",
        "  train_losses.append(avg_loss)\n",
        "\n",
        "  model.eval()\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for X,y in test_dataloader:\n",
        "      X = X.to(device)\n",
        "      y = y.to(device)\n",
        "      output = model(X)\n",
        "      _, pred = torch.max(output.data, 1)\n",
        "      total += y.size(0)\n",
        "      correct += (pred == y).sum().item()\n",
        "\n",
        "  accuracy = 100*correct/total\n",
        "  val_accuracies.append(accuracy)\n",
        "  print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Val Accuracy: {accuracy:.2f}%')"
      ],
      "metadata": {
        "id": "sjVYD2yS0jX_",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting results\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_results(train_losses, val_accuracies):\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Plotting Training loss curve\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(train_losses, label='Training Loss', color='tab:blue')\n",
        "    plt.title('Training Loss per Epoch')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.grid()\n",
        "    plt.legend()\n",
        "\n",
        "    # Plotting accuracy curve\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(val_accuracies, label='Validation Accuracy', color='tab:green')\n",
        "    plt.title('Validation Accuracy per Epoch')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.grid()\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "plot_results(train_losses, val_accuracies)\n",
        "\n"
      ],
      "metadata": {
        "id": "sW-ml_Hur_e6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}